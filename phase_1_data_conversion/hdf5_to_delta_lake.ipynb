{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6c5f9f",
   "metadata": {},
   "source": [
    "# **HDF5 to Delta Lake Data Conversion**\n",
    "The raw data files provided by Bosch are in HDF5 (.h5) format, a hierarchical data format commonly used for storing large numerical datasets, especially in scientific and industrial applications. While powerful for structured, multi-dimensional data, HDF5 is increasingly considered outdated for use in modern distributed data systems ‚Äî particularly due to its limited compatibility with big data tools and lack of built-in support for scalable query engines like Spark.\n",
    "\n",
    "The procedures used here can be easily adapted for other semi-structured formats such as CSV or JSON. The main logic of reading, transforming, and storing structured data remains the same.\n",
    "\n",
    "Our goal in this notebook is to:\n",
    "- Convert each .h5 file into a row-oriented format suitable for distributed processing ‚Äî in this case, Parquet files.\n",
    "- Store these files as a Delta Lake table, which provides fast, reliable access to large-scale data using Apache Spark.\n",
    "- Implement a partitioning strategy that enables efficient filtering and fast retrieval, based on the structure and metadata of the original files.\n",
    "\n",
    "This notebook represents Phase 1 of the pipeline: ingestion and conversion from raw industrial data into a format optimized for analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4e155",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note: Before running this notebook, make sure you have completed **Step 0.2** of the **Phase 0**. This involves downloading the raw data from the `CNC_Machining` repository and placing it in the data/raw/ directory. The code in this notebook relies on the presence of these .h5 files in that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041b7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f674a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where raw data is stored\n",
    "RAW_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "# Directory where Delta data is stored\n",
    "DELTA_DIR = os.path.join(\"..\", \"data\", \"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1b058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for logging any failed conversions from hdf5 to Delta\n",
    "FAILED_LOG = os.path.join(\"..\", \"data\", \"failed_files.txt\")\n",
    "SUCCESS_LOG = os.path.join(\"..\", \"data\", \"success_files.txt\")\n",
    "\n",
    "# Clean failed and success logs if they exists\n",
    "open(FAILED_LOG, \"w\").close()\n",
    "open(SUCCESS_LOG, \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b483750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for writing files to Delta\n",
    "MAX_WORKERS = 4  # You can increase this based on system resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0fa8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to store number of files per thread\n",
    "thread_data = threading.local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a858cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1702 .h5 files in the raw data directory.\n"
     ]
    }
   ],
   "source": [
    "# Check for hd5 files - fail if none found\n",
    "h5_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(RAW_DIR)\n",
    "    for f in files if f.endswith(\".h5\")\n",
    "]\n",
    "\n",
    "assert len(h5_files) > 0, \"‚ö†Ô∏è No .h5 files found in the raw data directory. Did you complete step 0.2?\"\n",
    "print(f\"‚úÖ Found {len(h5_files)} .h5 files in the raw data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79df61a",
   "metadata": {},
   "source": [
    "## üìä **Exploring Raw File Structure to Guide Delta Lake Partitioning**\n",
    "In this section, we collect metadata from each HDF5 file ‚Äî including dataset keys, shapes, and types ‚Äî to understand the structure and volume of data across files. This exploration helps us make an informed decision about how to partition the Delta Lake table. Partitioning is a way of organizing data by one or more key columns (e.g., machine ID, operation, or date) to improve query performance by enabling Spark to read only the relevant subsets of data. However, deep or overly granular partitioning (such as by exact timestamp or unique file ID) can backfire, creating many tiny files and directories that slow down query planning and metadata operations. Our goal is to find the right balance: enough partitioning to enable fast filtering, but not so much that it fragments the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0496d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata from HDF5 files and save to DataFrame\n",
    "def collect_hdf5_metadata(base_dir):\n",
    "    data = []\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".h5\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                record = {\n",
    "                    \"file_path\": file_path,\n",
    "                    \"num_rows\": None,\n",
    "                    \"num_columns\": None,\n",
    "                    \"column_names\": []\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as f:\n",
    "                        for key in f.keys():\n",
    "                            dataset = f[key]\n",
    "                            if len(dataset.shape) == 2:  # Ensure it's a 2D dataset\n",
    "                                record[\"num_rows\"] = dataset.shape[0]\n",
    "                                record[\"num_columns\"] = dataset.shape[1]\n",
    "                                record[\"column_names\"] = list(dataset.attrs.get(\"column_names\", []))\n",
    "                                break  # Process only the first 2D dataset\n",
    "                except Exception as e:\n",
    "                    record[\"error\"] = str(e)\n",
    "\n",
    "                data.append(record)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf720b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and save result\n",
    "df_files = collect_hdf5_metadata(RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c440a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>num_rows</th>\n",
       "      <th>num_columns</th>\n",
       "      <th>column_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/M01/OP03/bad/M01_Aug_2019_OP03_000.h5</td>\n",
       "      <td>139653</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...</td>\n",
       "      <td>179200</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Aug_2019_OP03_00...</td>\n",
       "      <td>178176</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Feb_2019_OP03_00...</td>\n",
       "      <td>156000</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...</td>\n",
       "      <td>178176</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  num_rows  num_columns  \\\n",
       "0  ../data/raw/M01/OP03/bad/M01_Aug_2019_OP03_000.h5    139653            3   \n",
       "1  ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...    179200            3   \n",
       "2  ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_00...    178176            3   \n",
       "3  ../data/raw/M01/OP03/good/M01_Feb_2019_OP03_00...    156000            3   \n",
       "4  ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...    178176            3   \n",
       "\n",
       "  column_names  \n",
       "0           []  \n",
       "1           []  \n",
       "2           []  \n",
       "3           []  \n",
       "4           []  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View summary\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eff0621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of rows and columns:\n",
      "num_rows       104936.098707\n",
      "num_columns         3.000000\n",
      "dtype: float64\n",
      "Number of files:\n",
      "1702\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean number of rows and columns:\")\n",
    "print(df_files[['num_rows', 'num_columns']].mean())\n",
    "print(\"Number of files:\")\n",
    "print(df_files['file_path'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba995331",
   "metadata": {},
   "source": [
    "## üìå **Data Structure and Partitioning Strategy**\n",
    "Each HDF5 file contains time-series sensor data with three columns: \"x\", \"y\", and \"z\", representing vibration signals along three axes. This structure is described in the original `CNC_Machining` [GitHub repository](https://github.com/boschresearch/CNC_Machining/blob/main/README.md).\n",
    "\n",
    "To efficiently store and query this data using Delta Lake, we will apply partitioning ‚Äî a strategy that organizes the dataset into subfolders based on selected column values. Partitioning allows Spark to read only the relevant slices of data when filtering, significantly improving performance on large datasets.\n",
    "\n",
    "Based on the file structure and metadata, we will partition the Delta table by:\n",
    "\n",
    "- `machine_id` (e.g., M01, M02)\n",
    "- `operation` (e.g., OP00, OP01)\n",
    "- `label` (e.g., good, bad)\n",
    "\n",
    "This strikes a balance between fast filtering and avoiding excessive small files or deeply nested directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2d9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "614ada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for the spark dataframe to ensure consistency\n",
    "# across all files\n",
    "schema = StructType([\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True),\n",
    "    StructField(\"machine_id\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"example_no\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec7ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse information from the file names\n",
    "def parse_filename(filename):\n",
    "    base = filename.replace(\".h5\", \"\")\n",
    "    parts = base.split(\"_\")\n",
    "    return {\n",
    "        \"machine_id\": parts[0],\n",
    "        \"month\": parts[1],\n",
    "        \"year\": parts[2],\n",
    "        \"operation\": parts[3],\n",
    "        \"example_no\": parts[4]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a982bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each file\n",
    "def process_file(file_path):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        # Initialize per-thread counter if not already set\n",
    "        if not hasattr(thread_data, \"file_count\"):\n",
    "            thread_data.file_count = 0\n",
    "\n",
    "        # Parse metadata from file name and path\n",
    "        meta = parse_filename(os.path.basename(file_path))\n",
    "        label = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "        # Check if already written to Delta\n",
    "        delta_df = spark.read.format(\"delta\").load(DELTA_DIR)\n",
    "        exists = delta_df.filter(\n",
    "            (col(\"machine_id\") == meta[\"machine_id\"]) &\n",
    "            (col(\"operation\") == meta[\"operation\"]) &\n",
    "            (col(\"example_no\") == meta[\"example_no\"]) &\n",
    "            (col(\"year\") == meta[\"year\"]) &\n",
    "            (col(\"month\") == meta[\"month\"]) &\n",
    "            (col(\"label\") == label)\n",
    "        ).limit(1).count() > 0\n",
    "\n",
    "        if exists:\n",
    "            duration = round(time.time() - start, 2)\n",
    "            return_msg = f\"‚è≠Ô∏è Skipped (already exists): {file_path} ({duration}s)\"\n",
    "        else:\n",
    "            with h5py.File(file_path, 'r') as h5f:\n",
    "                if \"vibration_data\" not in h5f:\n",
    "                    raise ValueError(\"Missing 'vibration_data'\")\n",
    "                data = h5f[\"vibration_data\"][()]\n",
    "                df_pd = pd.DataFrame(data, columns=[\"x\", \"y\", \"z\"])\n",
    "                df_pd[[\"x\", \"y\", \"z\"]] = df_pd[[\"x\", \"y\", \"z\"]].astype(\"float64\")\n",
    "\n",
    "                for k, v in meta.items():\n",
    "                    df_pd[k] = v\n",
    "                df_pd[\"label\"] = label\n",
    "\n",
    "                df_spark = spark.createDataFrame(df_pd, schema=schema)\n",
    "\n",
    "                df_spark.write.format(\"delta\") \\\n",
    "                    .partitionBy(\"machine_id\", \"operation\", \"label\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .save(DELTA_DIR)\n",
    "\n",
    "            # Manual cleanup only every 25 files per thread\n",
    "            thread_data.file_count += 1\n",
    "            if thread_data.file_count % 25 == 0:\n",
    "                del df_pd\n",
    "                del df_spark\n",
    "                spark.catalog.clearCache()\n",
    "                gc.collect()\n",
    "\n",
    "            duration = round(time.time() - start, 2)\n",
    "            return_msg = f\"‚úÖ {file_path} ({duration}s)\"\n",
    "\n",
    "        with open(SUCCESS_LOG, \"a\") as log:\n",
    "            log.write(f\"{return_msg}\\n\")\n",
    "\n",
    "        return return_msg\n",
    "\n",
    "    except Exception as e:\n",
    "        duration = round(time.time() - start, 2)\n",
    "        error_msg = f\"‚ö†Ô∏è {file_path} FAILED after {duration}s: {e}\"\n",
    "        with open(FAILED_LOG, \"a\") as log:\n",
    "            log.write(f\"{file_path} | {e}\\n\")\n",
    "        return error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3e9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of all .h5 files\n",
    "h5_files = []\n",
    "for root, _, files in os.walk(RAW_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".h5\"):\n",
    "            h5_files.append(os.path.join(root, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88244e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1702/1702 [1:05:23<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# To speed this up, we can use threading\n",
    "# Run threads with progress bar\n",
    "# Number of workers defined at top of file\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(process_file, f) for f in h5_files]\n",
    "    for result in tqdm(as_completed(futures), total=len(h5_files), desc=\"Processing files\"):\n",
    "        # Output was too much, so commented out this line\n",
    "        #print(result.result())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a696fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bcccb",
   "metadata": {},
   "source": [
    "**See the next notebook- `verify_delta_lake.ipynb`, to verify the data in the Delta Lake table is as expected.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
