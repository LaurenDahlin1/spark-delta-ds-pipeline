{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6c5f9f",
   "metadata": {},
   "source": [
    "# **HDF5 to Delta Lake Data Conversion**\n",
    "The raw data files provided by Bosch are in HDF5 (.h5) format, a hierarchical data format commonly used for storing large numerical datasets, especially in scientific and industrial applications. While powerful for structured, multi-dimensional data, HDF5 is increasingly considered outdated for use in modern distributed data systems ‚Äî particularly due to its limited compatibility with big data tools and lack of built-in support for scalable query engines like Spark.\n",
    "\n",
    "The procedures used here can be easily adapted for other semi-structured formats such as CSV or JSON. The main logic of reading, transforming, and storing structured data remains the same.\n",
    "\n",
    "Our goal in this notebook is to:\n",
    "- Convert each .h5 file into a row-oriented format suitable for distributed processing ‚Äî in this case, Parquet files.\n",
    "- Store these files as a Delta Lake table, which provides fast, reliable access to large-scale data using Apache Spark.\n",
    "- Implement a partitioning strategy that enables efficient filtering and fast retrieval, based on the structure and metadata of the original files.\n",
    "\n",
    "This notebook represents Phase 1 of the pipeline: ingestion and conversion from raw industrial data into a format optimized for analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4e155",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Note: Before running this notebook, make sure you have completed **Step 0.2** of the **Phase 0**. This involves downloading the raw data from the cnc_machines repository and placing it in the data/raw/ directory. The code in this notebook relies on the presence of these .h5 files in that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "041b7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where raw data is stored\n",
    "RAW_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "# Directory where Delta data is stored\n",
    "DELTA_DIR = os.path.join(\"..\", \"data\", \"delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba1b058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File for logging any failed conversions from hdf5 to Delta\n",
    "FAILED_LOG = os.path.join(\"..\", \"data\", \"failed_files.txt\")\n",
    "\n",
    "# Clean failed log if it exists\n",
    "open(FAILED_LOG, \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b483750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for writing files to Delta\n",
    "MAX_WORKERS = 4  # You can increase this based on system resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a858cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1702 .h5 files in the raw data directory.\n"
     ]
    }
   ],
   "source": [
    "# Check for hd5 files - fail if none found\n",
    "h5_files = [\n",
    "    os.path.join(root, f)\n",
    "    for root, _, files in os.walk(RAW_DIR)\n",
    "    for f in files if f.endswith(\".h5\")\n",
    "]\n",
    "\n",
    "assert len(h5_files) > 0, \"‚ö†Ô∏è No .h5 files found in the raw data directory. Did you complete step 0.2?\"\n",
    "print(f\"‚úÖ Found {len(h5_files)} .h5 files in the raw data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79df61a",
   "metadata": {},
   "source": [
    "## üìä **Exploring Raw File Structure to Guide Delta Lake Partitioning**\n",
    "In this section, we collect metadata from each HDF5 file ‚Äî including dataset keys, shapes, and types ‚Äî to understand the structure and volume of data across files. This exploration helps us make an informed decision about how to partition the Delta Lake table. Partitioning is a way of organizing data by one or more key columns (e.g., machine ID, operation, or date) to improve query performance by enabling Spark to read only the relevant subsets of data. However, deep or overly granular partitioning (such as by exact timestamp or unique file ID) can backfire, creating many tiny files and directories that slow down query planning and metadata operations. Our goal is to find the right balance: enough partitioning to enable fast filtering, but not so much that it fragments the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0496d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metadata from HDF5 files and save to DataFrame\n",
    "def collect_hdf5_metadata(base_dir):\n",
    "    data = []\n",
    "\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".h5\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                record = {\n",
    "                    \"file_path\": file_path,\n",
    "                    \"num_rows\": None,\n",
    "                    \"num_columns\": None,\n",
    "                    \"column_names\": []\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    with h5py.File(file_path, 'r') as f:\n",
    "                        for key in f.keys():\n",
    "                            dataset = f[key]\n",
    "                            if len(dataset.shape) == 2:  # Ensure it's a 2D dataset\n",
    "                                record[\"num_rows\"] = dataset.shape[0]\n",
    "                                record[\"num_columns\"] = dataset.shape[1]\n",
    "                                record[\"column_names\"] = list(dataset.attrs.get(\"column_names\", []))\n",
    "                                break  # Process only the first 2D dataset\n",
    "                except Exception as e:\n",
    "                    record[\"error\"] = str(e)\n",
    "\n",
    "                data.append(record)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cf720b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and save result\n",
    "df_files = collect_hdf5_metadata(RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c440a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>num_rows</th>\n",
       "      <th>num_columns</th>\n",
       "      <th>column_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/M01/OP03/bad/M01_Aug_2019_OP03_000.h5</td>\n",
       "      <td>139653</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...</td>\n",
       "      <td>179200</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Aug_2019_OP03_00...</td>\n",
       "      <td>178176</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Feb_2019_OP03_00...</td>\n",
       "      <td>156000</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...</td>\n",
       "      <td>178176</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path  num_rows  num_columns  \\\n",
       "0  ../data/raw/M01/OP03/bad/M01_Aug_2019_OP03_000.h5    139653            3   \n",
       "1  ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...    179200            3   \n",
       "2  ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_00...    178176            3   \n",
       "3  ../data/raw/M01/OP03/good/M01_Feb_2019_OP03_00...    156000            3   \n",
       "4  ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_00...    178176            3   \n",
       "\n",
       "  column_names  \n",
       "0           []  \n",
       "1           []  \n",
       "2           []  \n",
       "3           []  \n",
       "4           []  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View summary\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7eff0621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of rows and columns:\n",
      "num_rows       104936.098707\n",
      "num_columns         3.000000\n",
      "dtype: float64\n",
      "Number of files:\n",
      "1702\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean number of rows and columns:\")\n",
    "print(df_files[['num_rows', 'num_columns']].mean())\n",
    "print(\"Number of files:\")\n",
    "print(df_files['file_path'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba995331",
   "metadata": {},
   "source": [
    "## üìå **Data Structure and Partitioning Strategy**\n",
    "Each HDF5 file contains time-series sensor data with three columns: \"x\", \"y\", and \"z\", representing vibration signals along three axes. This structure is described in the original `CNC_machining` GitHub repository.\n",
    "\n",
    "To efficiently store and query this data using Delta Lake, we will apply partitioning ‚Äî a strategy that organizes the dataset into subfolders based on selected column values. Partitioning allows Spark to read only the relevant slices of data when filtering, significantly improving performance on large datasets.\n",
    "\n",
    "Based on the file structure and metadata, we will partition the Delta table by:\n",
    "\n",
    "- `machine_id` (e.g., M01, M02)\n",
    "- `operation` (e.g., OP00, OP01)\n",
    "- `label` (e.g., good, bad)\n",
    "\n",
    "This strikes a balance between fast filtering and avoiding excessive small files or deeply nested directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f2d9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "614ada95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a schema for the spark dataframe to ensure consistency\n",
    "# across all files\n",
    "schema = StructType([\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True),\n",
    "    StructField(\"machine_id\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"example_no\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ec7ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse information from the file names\n",
    "def parse_filename(filename):\n",
    "    base = filename.replace(\".h5\", \"\")\n",
    "    parts = base.split(\"_\")\n",
    "    return {\n",
    "        \"machine_id\": parts[0],\n",
    "        \"month\": parts[1],\n",
    "        \"year\": parts[2],\n",
    "        \"operation\": parts[3],\n",
    "        \"example_no\": parts[4]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a982bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: ../data/raw/M01/OP03/bad/M01_Aug_2019_OP03_000.h5 (11.12s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_000.h5 (7.58s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_008.h5 (8.49s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2019_OP03_002.h5 (7.89s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_001.h5 (8.26s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_009.h5 (8.1s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2021_OP03_003.h5 (12.14s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_006.h5 (9.9s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_012.h5 (7.51s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2021_OP03_007.h5 (7.67s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_002.h5 (8.12s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_013.h5 (7.41s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2021_OP03_006.h5 (8.44s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_003.h5 (9.51s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2021_OP03_002.h5 (8.31s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_007.h5 (7.01s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_010.h5 (8.56s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2021_OP03_005.h5 (6.98s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_000.h5 (7.07s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_004.h5 (6.64s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_005.h5 (6.95s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_011.h5 (8.36s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2021_OP03_004.h5 (8.66s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Aug_2019_OP03_001.h5 (7.86s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_002.h5 (7.67s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2019_OP03_000.h5 (7.55s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2020_OP03_003.h5 (8.0s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP03/good/M01_Feb_2019_OP03_001.h5 (9.09s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/bad/M01_Feb_2019_OP04_000.h5 (7.02s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/bad/M01_Aug_2019_OP04_000.h5 (4.55s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2019_OP04_000.h5 (7.84s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2021_OP04_001.h5 (7.19s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2019_OP04_004.h5 (8.12s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2021_OP04_000.h5 (8.83s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2019_OP04_001.h5 (8.12s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_010.h5 (6.97s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_005.h5 (7.79s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_000.h5 (6.96s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_014.h5 (6.64s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_011.h5 (6.76s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_001.h5 (7.72s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_004.h5 (6.58s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_010.h5 (6.79s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_000.h5 (6.4s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_005.h5 (6.22s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_011.h5 (7.32s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_004.h5 (7.65s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_001.h5 (6.29s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_003.h5 (6.16s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_006.h5 (6.13s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_012.h5 (5.65s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_007.h5 (6.1s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_002.h5 (5.97s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_013.h5 (7.84s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_006.h5 (8.33s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_003.h5 (7.17s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_002.h5 (7.38s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_007.h5 (6.35s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_009.h5 (6.75s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_008.h5 (5.49s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2019_OP04_002.h5 (6.74s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Aug_2019_OP04_009.h5 (6.36s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2019_OP04_003.h5 (6.13s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP04/good/M01_Feb_2021_OP04_008.h5 (8.14s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP05/bad/M01_Feb_2019_OP05_000.h5 (7.52s)\n",
      "‚úÖ Saved: ../data/raw/M01/OP05/bad/M01_Feb_2019_OP05_001.h5 (6.34s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert to Spark and write\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df_spark \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(df_pd, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m     24\u001b[0m \u001b[43mdf_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmachine_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moperation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDELTA_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stream each file\n",
    "for root, _, files in os.walk(RAW_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith(\".h5\"):\n",
    "            file_path = os.path.join(root, f)\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as h5f:\n",
    "                    if \"vibration_data\" not in h5f:\n",
    "                        continue\n",
    "                    data = h5f[\"vibration_data\"][()]\n",
    "                    df_pd = pd.DataFrame(data, columns=[\"x\", \"y\", \"z\"])\n",
    "                    df_pd[[\"x\", \"y\", \"z\"]] = df_pd[[\"x\", \"y\", \"z\"]].astype(\"float64\")\n",
    "\n",
    "                    meta = parse_filename(f)\n",
    "                    label = os.path.basename(os.path.dirname(file_path))\n",
    "                    for k, v in meta.items():\n",
    "                        df_pd[k] = v\n",
    "                    df_pd[\"label\"] = label\n",
    "\n",
    "                    # Convert to Spark and write\n",
    "                    df_spark = spark.createDataFrame(df_pd, schema=schema)\n",
    "\n",
    "                    df_spark.write.format(\"delta\") \\\n",
    "                        .partitionBy(\"machine_id\", \"operation\", \"label\") \\\n",
    "                        .mode(\"append\") \\\n",
    "                        .save(DELTA_DIR)\n",
    "\n",
    "                    duration = round(time.time() - start_time, 2)\n",
    "                    print(f\"‚úÖ Saved: {file_path} ({duration}s)\")\n",
    "\n",
    "            except Exception as e:\n",
    "                duration = round(time.time() - start_time, 2)\n",
    "                print(f\"‚ö†Ô∏è Error reading {file_path} after {duration}s: {e}\")\n",
    "                with open(FAILED_LOG, \"a\") as log:\n",
    "                    log.write(f\"{file_path} | {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a696fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
